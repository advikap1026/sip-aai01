{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xQe1kYTobZB",
        "outputId": "aa18513a-3e77-4c35-9f12-707d0898589b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10),\n",
        "            #nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqpOQMD5q16P",
        "outputId": "7e628be6-1fd9-44f1-feb9-3d4c4b90b934"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "kOWTT-4KrDGD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUkzCLH6rLo3",
        "outputId": "c0aa22f6-6390-48da-f881-0f1cbfb18328"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308794  [   64/60000]\n",
            "loss: 0.192188  [ 6464/60000]\n",
            "loss: 0.158445  [12864/60000]\n",
            "loss: 0.272624  [19264/60000]\n",
            "loss: 0.120270  [25664/60000]\n",
            "loss: 0.278927  [32064/60000]\n",
            "loss: 0.151959  [38464/60000]\n",
            "loss: 0.164770  [44864/60000]\n",
            "loss: 0.150469  [51264/60000]\n",
            "loss: 0.137690  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.9%, Avg loss: 0.127391 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.049201  [   64/60000]\n",
            "loss: 0.122927  [ 6464/60000]\n",
            "loss: 0.088134  [12864/60000]\n",
            "loss: 0.157846  [19264/60000]\n",
            "loss: 0.063457  [25664/60000]\n",
            "loss: 0.201452  [32064/60000]\n",
            "loss: 0.095341  [38464/60000]\n",
            "loss: 0.082901  [44864/60000]\n",
            "loss: 0.118951  [51264/60000]\n",
            "loss: 0.074138  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.8%, Avg loss: 0.106873 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.012308  [   64/60000]\n",
            "loss: 0.151527  [ 6464/60000]\n",
            "loss: 0.029233  [12864/60000]\n",
            "loss: 0.093036  [19264/60000]\n",
            "loss: 0.043250  [25664/60000]\n",
            "loss: 0.066340  [32064/60000]\n",
            "loss: 0.062631  [38464/60000]\n",
            "loss: 0.013251  [44864/60000]\n",
            "loss: 0.085688  [51264/60000]\n",
            "loss: 0.014280  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.094955 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.054309  [   64/60000]\n",
            "loss: 0.040910  [ 6464/60000]\n",
            "loss: 0.027968  [12864/60000]\n",
            "loss: 0.089532  [19264/60000]\n",
            "loss: 0.009218  [25664/60000]\n",
            "loss: 0.010936  [32064/60000]\n",
            "loss: 0.015750  [38464/60000]\n",
            "loss: 0.008769  [44864/60000]\n",
            "loss: 0.049044  [51264/60000]\n",
            "loss: 0.018908  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.100621 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.022056  [   64/60000]\n",
            "loss: 0.005735  [ 6464/60000]\n",
            "loss: 0.089571  [12864/60000]\n",
            "loss: 0.036716  [19264/60000]\n",
            "loss: 0.005131  [25664/60000]\n",
            "loss: 0.047980  [32064/60000]\n",
            "loss: 0.018450  [38464/60000]\n",
            "loss: 0.051308  [44864/60000]\n",
            "loss: 0.091061  [51264/60000]\n",
            "loss: 0.003488  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.111881 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.032645  [   64/60000]\n",
            "loss: 0.062694  [ 6464/60000]\n",
            "loss: 0.024984  [12864/60000]\n",
            "loss: 0.029029  [19264/60000]\n",
            "loss: 0.009435  [25664/60000]\n",
            "loss: 0.015077  [32064/60000]\n",
            "loss: 0.001199  [38464/60000]\n",
            "loss: 0.030329  [44864/60000]\n",
            "loss: 0.013420  [51264/60000]\n",
            "loss: 0.001675  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.103252 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.003816  [   64/60000]\n",
            "loss: 0.036788  [ 6464/60000]\n",
            "loss: 0.018412  [12864/60000]\n",
            "loss: 0.025960  [19264/60000]\n",
            "loss: 0.036312  [25664/60000]\n",
            "loss: 0.006257  [32064/60000]\n",
            "loss: 0.033704  [38464/60000]\n",
            "loss: 0.020270  [44864/60000]\n",
            "loss: 0.206806  [51264/60000]\n",
            "loss: 0.019064  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.103045 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.001308  [   64/60000]\n",
            "loss: 0.000952  [ 6464/60000]\n",
            "loss: 0.002485  [12864/60000]\n",
            "loss: 0.007404  [19264/60000]\n",
            "loss: 0.000320  [25664/60000]\n",
            "loss: 0.024725  [32064/60000]\n",
            "loss: 0.016506  [38464/60000]\n",
            "loss: 0.003641  [44864/60000]\n",
            "loss: 0.083726  [51264/60000]\n",
            "loss: 0.010331  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.120925 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.005668  [   64/60000]\n",
            "loss: 0.012143  [ 6464/60000]\n",
            "loss: 0.000500  [12864/60000]\n",
            "loss: 0.000393  [19264/60000]\n",
            "loss: 0.000341  [25664/60000]\n",
            "loss: 0.018165  [32064/60000]\n",
            "loss: 0.001789  [38464/60000]\n",
            "loss: 0.008641  [44864/60000]\n",
            "loss: 0.033842  [51264/60000]\n",
            "loss: 0.006353  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.107754 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.006019  [   64/60000]\n",
            "loss: 0.001986  [ 6464/60000]\n",
            "loss: 0.006418  [12864/60000]\n",
            "loss: 0.016920  [19264/60000]\n",
            "loss: 0.000227  [25664/60000]\n",
            "loss: 0.106427  [32064/60000]\n",
            "loss: 0.019083  [38464/60000]\n",
            "loss: 0.001739  [44864/60000]\n",
            "loss: 0.047088  [51264/60000]\n",
            "loss: 0.011793  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.116214 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.027968  [   64/60000]\n",
            "loss: 0.001310  [ 6464/60000]\n",
            "loss: 0.001258  [12864/60000]\n",
            "loss: 0.000055  [19264/60000]\n",
            "loss: 0.002065  [25664/60000]\n",
            "loss: 0.005723  [32064/60000]\n",
            "loss: 0.003460  [38464/60000]\n",
            "loss: 0.020448  [44864/60000]\n",
            "loss: 0.010557  [51264/60000]\n",
            "loss: 0.008823  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.132376 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.005500  [   64/60000]\n",
            "loss: 0.000205  [ 6464/60000]\n",
            "loss: 0.020091  [12864/60000]\n",
            "loss: 0.011272  [19264/60000]\n",
            "loss: 0.002343  [25664/60000]\n",
            "loss: 0.006060  [32064/60000]\n",
            "loss: 0.006123  [38464/60000]\n",
            "loss: 0.115340  [44864/60000]\n",
            "loss: 0.013384  [51264/60000]\n",
            "loss: 0.000633  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.111464 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.000044  [   64/60000]\n",
            "loss: 0.029203  [ 6464/60000]\n",
            "loss: 0.009382  [12864/60000]\n",
            "loss: 0.001972  [19264/60000]\n",
            "loss: 0.001675  [25664/60000]\n",
            "loss: 0.002914  [32064/60000]\n",
            "loss: 0.001909  [38464/60000]\n",
            "loss: 0.004049  [44864/60000]\n",
            "loss: 0.043653  [51264/60000]\n",
            "loss: 0.021501  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.127795 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.000076  [   64/60000]\n",
            "loss: 0.006543  [ 6464/60000]\n",
            "loss: 0.014072  [12864/60000]\n",
            "loss: 0.000074  [19264/60000]\n",
            "loss: 0.020932  [25664/60000]\n",
            "loss: 0.005099  [32064/60000]\n",
            "loss: 0.001309  [38464/60000]\n",
            "loss: 0.000009  [44864/60000]\n",
            "loss: 0.001637  [51264/60000]\n",
            "loss: 0.026998  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.113547 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.000417  [   64/60000]\n",
            "loss: 0.006391  [ 6464/60000]\n",
            "loss: 0.001811  [12864/60000]\n",
            "loss: 0.036675  [19264/60000]\n",
            "loss: 0.001318  [25664/60000]\n",
            "loss: 0.007311  [32064/60000]\n",
            "loss: 0.066223  [38464/60000]\n",
            "loss: 0.000023  [44864/60000]\n",
            "loss: 0.000456  [51264/60000]\n",
            "loss: 0.002563  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.129091 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.000783  [   64/60000]\n",
            "loss: 0.008538  [ 6464/60000]\n",
            "loss: 0.023994  [12864/60000]\n",
            "loss: 0.000039  [19264/60000]\n",
            "loss: 0.000064  [25664/60000]\n",
            "loss: 0.002158  [32064/60000]\n",
            "loss: 0.000099  [38464/60000]\n",
            "loss: 0.000007  [44864/60000]\n",
            "loss: 0.032925  [51264/60000]\n",
            "loss: 0.037869  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.138170 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.031408  [   64/60000]\n",
            "loss: 0.000001  [ 6464/60000]\n",
            "loss: 0.066613  [12864/60000]\n",
            "loss: 0.006166  [19264/60000]\n",
            "loss: 0.008979  [25664/60000]\n",
            "loss: 0.000051  [32064/60000]\n",
            "loss: 0.000171  [38464/60000]\n",
            "loss: 0.024167  [44864/60000]\n",
            "loss: 0.000534  [51264/60000]\n",
            "loss: 0.000266  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.179314 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.002199  [   64/60000]\n",
            "loss: 0.001444  [ 6464/60000]\n",
            "loss: 0.000352  [12864/60000]\n",
            "loss: 0.000018  [19264/60000]\n",
            "loss: 0.001461  [25664/60000]\n",
            "loss: 0.032300  [32064/60000]\n",
            "loss: 0.099436  [38464/60000]\n",
            "loss: 0.000063  [44864/60000]\n",
            "loss: 0.000561  [51264/60000]\n",
            "loss: 0.000073  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.122226 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.002744  [   64/60000]\n",
            "loss: 0.001637  [ 6464/60000]\n",
            "loss: 0.013484  [12864/60000]\n",
            "loss: 0.000008  [19264/60000]\n",
            "loss: 0.000067  [25664/60000]\n",
            "loss: 0.000007  [32064/60000]\n",
            "loss: 0.000014  [38464/60000]\n",
            "loss: 0.000035  [44864/60000]\n",
            "loss: 0.015993  [51264/60000]\n",
            "loss: 0.003261  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.125192 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.000293  [   64/60000]\n",
            "loss: 0.012070  [ 6464/60000]\n",
            "loss: 0.003107  [12864/60000]\n",
            "loss: 0.000063  [19264/60000]\n",
            "loss: 0.000151  [25664/60000]\n",
            "loss: 0.000025  [32064/60000]\n",
            "loss: 0.000001  [38464/60000]\n",
            "loss: 0.000164  [44864/60000]\n",
            "loss: 0.054067  [51264/60000]\n",
            "loss: 0.000027  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.138840 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.000011  [   64/60000]\n",
            "loss: 0.000565  [ 6464/60000]\n",
            "loss: 0.001353  [12864/60000]\n",
            "loss: 0.000115  [19264/60000]\n",
            "loss: 0.010014  [25664/60000]\n",
            "loss: 0.010103  [32064/60000]\n",
            "loss: 0.000542  [38464/60000]\n",
            "loss: 0.000256  [44864/60000]\n",
            "loss: 0.006181  [51264/60000]\n",
            "loss: 0.000847  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.120927 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.000767  [   64/60000]\n",
            "loss: 0.000056  [ 6464/60000]\n",
            "loss: 0.007767  [12864/60000]\n",
            "loss: 0.000653  [19264/60000]\n",
            "loss: 0.000028  [25664/60000]\n",
            "loss: 0.000018  [32064/60000]\n",
            "loss: 0.000006  [38464/60000]\n",
            "loss: 0.000045  [44864/60000]\n",
            "loss: 0.128039  [51264/60000]\n",
            "loss: 0.000028  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.139430 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.002743  [   64/60000]\n",
            "loss: 0.000044  [ 6464/60000]\n",
            "loss: 0.005971  [12864/60000]\n",
            "loss: 0.025138  [19264/60000]\n",
            "loss: 0.000515  [25664/60000]\n",
            "loss: 0.082413  [32064/60000]\n",
            "loss: 0.000002  [38464/60000]\n",
            "loss: 0.031142  [44864/60000]\n",
            "loss: 0.000869  [51264/60000]\n",
            "loss: 0.000045  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.4%, Avg loss: 0.117196 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.000428  [   64/60000]\n",
            "loss: 0.002242  [ 6464/60000]\n",
            "loss: 0.000004  [12864/60000]\n",
            "loss: 0.000001  [19264/60000]\n",
            "loss: 0.000004  [25664/60000]\n",
            "loss: 0.000013  [32064/60000]\n",
            "loss: 0.000833  [38464/60000]\n",
            "loss: 0.000649  [44864/60000]\n",
            "loss: 0.001110  [51264/60000]\n",
            "loss: 0.000024  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.126889 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.003170  [   64/60000]\n",
            "loss: 0.017671  [ 6464/60000]\n",
            "loss: 0.000057  [12864/60000]\n",
            "loss: 0.001548  [19264/60000]\n",
            "loss: 0.001159  [25664/60000]\n",
            "loss: 0.000010  [32064/60000]\n",
            "loss: 0.000001  [38464/60000]\n",
            "loss: 0.000062  [44864/60000]\n",
            "loss: 0.034834  [51264/60000]\n",
            "loss: 0.010628  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.8%, Avg loss: 0.148799 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}